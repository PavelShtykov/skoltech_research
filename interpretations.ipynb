{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_fixed import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bertviz import head_view\n",
    "from bertviz.neuron_view import show\n",
    "from datasets import load_dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f2c591ffa34d548e030621560430df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model_name = \"EleutherAI/llemma_7b\" # Support\n",
    "model_name = 'huggyllama/llama-7b' # Support\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "# model.eval()\n",
    "\n",
    "mistral_name = 'mistralai/Mistral-7B-v0.1' # Currently out of support :(\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_name)\n",
    "mistral_tokenizer.pad_token = \"[PAD]\"\n",
    "mistral_tokenizer.padding_side = \"left\"\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(mistral_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "mistral_model.eval()\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "# def ret_queries(sent, model=model):\n",
    "#     \"\"\"\n",
    "#     return (n_layers, n_heads, emb_query)\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer(sent, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     # splitted_text = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "#     model.forward(**inputs)\n",
    "#     qk_dict = model.get_qk_dict()\n",
    "#     queries = torch.stack([qk['query'] for l, qk in qk_dict.items()]).squeeze()\n",
    "\n",
    "#     return queries\n",
    "\n",
    "# def ret_keys(sent, model=model):\n",
    "#     \"\"\"\n",
    "#     return (n_layers, n_heads, emb_query)\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer(sent, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     # splitted_text = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "#     model.forward(**inputs)\n",
    "#     qk_dict = model.get_qk_dict()\n",
    "#     queries = torch.stack([qk['key'] for l, qk in qk_dict.items()]).squeeze()\n",
    "\n",
    "#     return queries\n",
    "\n",
    "\n",
    "# def find_token_idx(tok: str, sent: str):\n",
    "#     inp = tokenizer(sent, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     tok_list = tokenizer.convert_ids_to_tokens(inp['input_ids'][0])\n",
    "#     # print(tok_list)\n",
    "#     return tok_list.index(tok)\n",
    "# inp\n",
    "\n",
    "\n",
    "# model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sents, model=mistral_model, tokenizer=mistral_tokenizer):\n",
    "    inputs = tokenizer(sents, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    output_sequences = model.generate(**inputs, max_new_tokens=200, use_cache=True, do_sample=True, top_k=10, eos_token_id=tokenizer.eos_token_id).to('cpu')\n",
    "\n",
    "    return tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>The_Legend_of_Zelda:_Twilight_Princess</td>\n",
       "      <td>A CD containing 20 musical selections from the...</td>\n",
       "      <td>What company included the soundtrack as a rewa...</td>\n",
       "      <td>[{'text': 'GameStop', 'answer_start': 71}]</td>\n",
       "      <td>56cda64a62d2951400fa67be</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>The_Legend_of_Zelda:_Twilight_Princess</td>\n",
       "      <td>A CD containing 20 musical selections from the...</td>\n",
       "      <td>How many tracks were recorded on the preorder CD?</td>\n",
       "      <td>[{'text': '20', 'answer_start': 16}]</td>\n",
       "      <td>56cda64a62d2951400fa67bf</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>The_Legend_of_Zelda:_Twilight_Princess</td>\n",
       "      <td>A CD containing 20 musical selections from the...</td>\n",
       "      <td>In what areas is the content of the GameStop b...</td>\n",
       "      <td>[{'text': 'Japan, Europe, and Australia', 'ans...</td>\n",
       "      <td>56cda64a62d2951400fa67c0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>The_Legend_of_Zelda:_Twilight_Princess</td>\n",
       "      <td>A CD containing 20 musical selections from the...</td>\n",
       "      <td>What was included as a Gamestop preorder item?</td>\n",
       "      <td>[{'text': 'CD', 'answer_start': 2}]</td>\n",
       "      <td>56d13400e7d4791d00901fdd</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>The_Legend_of_Zelda:_Twilight_Princess</td>\n",
       "      <td>A CD containing 20 musical selections from the...</td>\n",
       "      <td>What company included the soundtrack as a rewa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5a8dbd49df8bba001a0f9bb5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128102</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>Anthropology of development tends to view deve...</td>\n",
       "      <td>What does a lot of planned development apparen...</td>\n",
       "      <td>[{'text': 'fail', 'answer_start': 527}]</td>\n",
       "      <td>5733cd1c4776f4190066127e</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128103</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>Anthropology of development tends to view deve...</td>\n",
       "      <td>What tends to view development from a positive...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5ad2ee84604f3c001a3fd9ef</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128104</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>Anthropology of development tends to view deve...</td>\n",
       "      <td>What field of anthropology has a goal to elevi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5ad2ee84604f3c001a3fd9f0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128105</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>Anthropology of development tends to view deve...</td>\n",
       "      <td>What looks for the connections between plans a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5ad2ee84604f3c001a3fd9f1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128106</th>\n",
       "      <td>Anthropology</td>\n",
       "      <td>Anthropology of development tends to view deve...</td>\n",
       "      <td>What type of development rarely fails?</td>\n",
       "      <td>[]</td>\n",
       "      <td>5ad2ee84604f3c001a3fd9f2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "2375    The_Legend_of_Zelda:_Twilight_Princess   \n",
       "2376    The_Legend_of_Zelda:_Twilight_Princess   \n",
       "2377    The_Legend_of_Zelda:_Twilight_Princess   \n",
       "2378    The_Legend_of_Zelda:_Twilight_Princess   \n",
       "2379    The_Legend_of_Zelda:_Twilight_Princess   \n",
       "...                                        ...   \n",
       "128102                            Anthropology   \n",
       "128103                            Anthropology   \n",
       "128104                            Anthropology   \n",
       "128105                            Anthropology   \n",
       "128106                            Anthropology   \n",
       "\n",
       "                                                  context  \\\n",
       "2375    A CD containing 20 musical selections from the...   \n",
       "2376    A CD containing 20 musical selections from the...   \n",
       "2377    A CD containing 20 musical selections from the...   \n",
       "2378    A CD containing 20 musical selections from the...   \n",
       "2379    A CD containing 20 musical selections from the...   \n",
       "...                                                   ...   \n",
       "128102  Anthropology of development tends to view deve...   \n",
       "128103  Anthropology of development tends to view deve...   \n",
       "128104  Anthropology of development tends to view deve...   \n",
       "128105  Anthropology of development tends to view deve...   \n",
       "128106  Anthropology of development tends to view deve...   \n",
       "\n",
       "                                                 question  \\\n",
       "2375    What company included the soundtrack as a rewa...   \n",
       "2376    How many tracks were recorded on the preorder CD?   \n",
       "2377    In what areas is the content of the GameStop b...   \n",
       "2378       What was included as a Gamestop preorder item?   \n",
       "2379    What company included the soundtrack as a rewa...   \n",
       "...                                                   ...   \n",
       "128102  What does a lot of planned development apparen...   \n",
       "128103  What tends to view development from a positive...   \n",
       "128104  What field of anthropology has a goal to elevi...   \n",
       "128105  What looks for the connections between plans a...   \n",
       "128106             What type of development rarely fails?   \n",
       "\n",
       "                                                   answer  \\\n",
       "2375           [{'text': 'GameStop', 'answer_start': 71}]   \n",
       "2376                 [{'text': '20', 'answer_start': 16}]   \n",
       "2377    [{'text': 'Japan, Europe, and Australia', 'ans...   \n",
       "2378                  [{'text': 'CD', 'answer_start': 2}]   \n",
       "2379                                                   []   \n",
       "...                                                   ...   \n",
       "128102            [{'text': 'fail', 'answer_start': 527}]   \n",
       "128103                                                 []   \n",
       "128104                                                 []   \n",
       "128105                                                 []   \n",
       "128106                                                 []   \n",
       "\n",
       "                              id  is_impossible  \n",
       "2375    56cda64a62d2951400fa67be          False  \n",
       "2376    56cda64a62d2951400fa67bf          False  \n",
       "2377    56cda64a62d2951400fa67c0          False  \n",
       "2378    56d13400e7d4791d00901fdd          False  \n",
       "2379    5a8dbd49df8bba001a0f9bb5           True  \n",
       "...                          ...            ...  \n",
       "128102  5733cd1c4776f4190066127e          False  \n",
       "128103  5ad2ee84604f3c001a3fd9ef           True  \n",
       "128104  5ad2ee84604f3c001a3fd9f0           True  \n",
       "128105  5ad2ee84604f3c001a3fd9f1           True  \n",
       "128106  5ad2ee84604f3c001a3fd9f2           True  \n",
       "\n",
       "[795 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./squad/train-v2.0.json', 'rt') as f:\n",
    "    ds = json.load(f)\n",
    "\n",
    "# Parsing json to DataFrame\n",
    "\n",
    "res = []\n",
    "for curr in ds['data']:\n",
    "    title = curr['title']\n",
    "    for par in curr['paragraphs']:\n",
    "        context = par['context']\n",
    "        for qa in par['qas']:\n",
    "            question = qa['question']\n",
    "            id = qa['id']\n",
    "            ans = qa['answers']\n",
    "            is_imposs = qa['is_impossible']\n",
    "\n",
    "            res.append([title, context, question, ans, id, is_imposs])\n",
    "        \n",
    "df = pd.DataFrame(res, columns=['title', 'context', 'question', 'answer', 'id', 'is_impossible'])\n",
    "\n",
    "# Filtering \"nice\" contexts and questions\n",
    "\n",
    "# Context has only one '.' symbol \n",
    "sub_df = df.loc[df['context'].isin(list(filter(lambda x: x.count('.') == 1, df['context'].unique()))), :]\n",
    "\n",
    "# Question has only one '?' symbol\n",
    "sub_df = sub_df.loc[sub_df['question'].apply(lambda x: x.count('?')) == 1, :]\n",
    "\n",
    "# Handcrafted feature (?): Context should has at least 1 impossible question and at lest 4 possible questions\n",
    "count_df = sub_df.groupby('context').agg({'is_impossible': ['sum', 'count']})\n",
    "nice_cont = count_df.loc[(count_df['is_impossible']['sum'] >= 1) & (count_df['is_impossible']['count'] - count_df['is_impossible']['sum'] >= 4) , :].index\n",
    "print(len(nice_cont))\n",
    "\n",
    "# Filter such \"nice\" contexts\n",
    "squad = df.loc[df['context'].isin(nice_cont), :]\n",
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnli = load_dataset('glue', 'mnli')\n",
    "# mnli = mnli['train'].to_pandas()\n",
    "\n",
    "# group = mnli.groupby('premise').agg({'hypothesis': 'count', 'label': pd.Series.nunique})\n",
    "# cont1 = group.loc[(group['hypothesis'] > 3) & (group['label'] == 3), :].index\n",
    "\n",
    "# mnli_filtred = mnli.loc[mnli['premise'].isin(cont1), :]\n",
    "# print(mnli_filtred.shape, mnli.shape)\n",
    "\n",
    "# mnli = mnli_filtred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = (mnli['premise'] + ' ' + mnli['hypothesis']).apply(lambda s: tokenizer(s)['input_ids'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(aa, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (aa < 15).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnli_part = mnli.loc[aa.between(15, 20), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mnli_part_sent.pkl', 'wb') as f:\n",
    "#     pickle.dump(mnli_part_sent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['How do you know? All this is their information again. This information belongs to them.',\n",
       "  'Issues in Data Synthesis. Problems in data synthesis.',\n",
       "  'well you see that on television also You can see that on television, as well.',\n",
       "  'The other men shuffled. The other men were shuffled around.'],\n",
       " 33971)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnli_part_sent = mnli_part['premise'] + ' ' + mnli_part['hypothesis']\n",
    "# mnli_part_sent = mnli_part_sent.to_list()\n",
    "\n",
    "\n",
    "with open('mnli_part_sent.pkl', 'rb') as f:\n",
    "    mnli_part_sent = pickle.load(f)\n",
    "\n",
    "mnli_part_sent[:4], len(mnli_part_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "mnli_part_sent_10k = random.sample(mnli_part_sent, 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnli_part_sent_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_att = []\n",
    "# all_tokens = []\n",
    "# bs = 32\n",
    "# for i in tqdm(range(0, len(mnli_part_sent_10k), bs)):\n",
    "#     curr_sents = mnli_part_sent[i:i+bs]\n",
    "#     inputs = tokenizer(curr_sents, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "#     res = model.forward(**inputs, output_attentions=True)\n",
    "\n",
    "#     all_tokens.append(inputs['input_ids'].cpu())\n",
    "#     real_att.append(torch.stack(res.attentions).detach().cpu())\n",
    "#     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_att = list(filter(lambda s: s.shape[-1] == 20, real_att))\n",
    "# real_att_merge = torch.concat(real_att, dim=1)\n",
    "# del real_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_att_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_att_merge = real_att_merge.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(real_att_merge, 'real_att_merge.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_att_merge = torch.load('real_att_merge.pt')[:, :10_000, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens = list(filter(lambda x: x.shape[1] == 20, all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens_merge = torch.concat(all_tokens, dim=0)\n",
    "# del all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens_merge = all_tokens_merge.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(all_tokens_merge, 'all_tokens_merge.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_merge = torch.load('all_tokens_merge.pt')[:10_000, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def templ(att, tokens, tokenizer=tokenizer):\n",
    "    mask = tokens != 0\n",
    "    tokens = tokens[mask]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    tokens = list(map(lambda x: x.replace('▁', ''), tokens))\n",
    "\n",
    "    output = ['<start>']\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(0, i):\n",
    "            output.append(f\"{tokens[i]} -> {tokens[j]}\\t{att[i, j]}\")\n",
    "        output.append('')\n",
    "\n",
    "    output.append('<end>')\n",
    "    output = '\\n'.join(output)\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 4177, 29879], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Gods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_att = (real_att_merge * 100).to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_n = 31\n",
    "head_n = 29\n",
    "i = 500\n",
    "\n",
    "templ(map_att[layer_n, i, head_n, ...], all_tokens_merge[i, ...])\n",
    "tokenizer(templ(map_att[layer_n, i, head_n, ...], all_tokens_merge[i, ...]))['input_ids'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_exp = \"\"\"We're studying self-attention heads in transformer large language model. Each head looks for some particular relationship between tokens in a short document. Look at the attention matrix for the part of the document and summarize in a single sentence what the head is looking for. Don't list examples of words.\n",
    "\n",
    "The attention matrix format is tokenA -> tokenB<tab>attention_score. Attention score values range from 0 to 100. A head finding what it's looking for is represented by a non-zero attention score. The higher the attention score, the stronger the match.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = {\n",
    "    templ(map_att[10, 3, 10, ...], all_tokens_merge[3, ...]): 'related to first subject',\n",
    "    templ(map_att[15, 130, 15, ...], all_tokens_merge[100, ...]): 'related to capital article',\n",
    "    templ(map_att[31, 500, 29, ...], all_tokens_merge[500, ...]): 'related to the main characters'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_prompt_exp(curr_attn, curr_tokens, few_shot, all_attn=map_att, all_tokens=all_tokens_merge):\n",
    "    res = ''\n",
    "    res += prompt_exp + '\\n\\n'\n",
    "\n",
    "    for i, kv in enumerate(few_shot.items(), 1):\n",
    "        k, v = kv\n",
    "        res += f'Head {i}\\nAttention scores:\\n'\n",
    "        res += k + '\\n\\n'\n",
    "        res += f'Explanation of head {i} behavior: the main thing this head does is find relationship {v}.\\n\\n'\n",
    "\n",
    "    \n",
    "    res += f'Head {i+1}\\nAttention scores:\\n'\n",
    "    res += templ(curr_attn, curr_tokens) + '\\n\\n'\n",
    "    res += f'Explanation of head {i+1} behavior: the main thing this head does is find relationship '\n",
    "\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_temp = make_full_prompt_exp(map_att[23, 11, 11, ...], all_tokens_merge[11, ...], few_shot)\n",
    "# print(exp_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sim = \"\"\"We're studying self-attention heads in transformer large language model. Each head looks for some particular relationship between tokens in a short document. Look at an explanation of what the head does, and try to predict how it will split attention between tokens.\n",
    "\n",
    "The attention matrix format is tokenA -> tokenB<tab>attention_score. Attention score values range from 0 to 100, \"unknown\" indicates an unknown attention score. A head finding what it's looking for is represented by a non-zero attention score. The higher the attention score, the stronger the match.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_prompt_sim(curr_exp, curr_tokens, few_shot=few_shot, all_attn=map_att, all_tokens=all_tokens_merge):\n",
    "    res = ''\n",
    "    res += prompt_sim + '\\n\\n'\n",
    "\n",
    "    for i, kv in enumerate(few_shot.items(), 1):\n",
    "        k, v = kv\n",
    "        res += f'Head {i}\\n'\n",
    "        res += f'Explanation of head {i} behavior: the main thing this head does is find relationship {v}.\\n'\n",
    "        res += 'Attention scores:\\n'\n",
    "        res += k + '\\n\\n'\n",
    "\n",
    "    curr_attn = ['unknown'] * len(curr_tokens)\n",
    "    curr_attn = [curr_attn] * len(curr_tokens)\n",
    "    curr_attn = np.array(curr_attn)\n",
    "    res += f'Head {i+1}\\n'\n",
    "    res += f'Explanation of head {i+1} behavior: the main thing this head does is find relationship {curr_exp}.\\n'\n",
    "\n",
    "    res += 'Attention scores:\\n'\n",
    "    res += templ(curr_attn, curr_tokens)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_temp = make_full_prompt_sim('sdfssd sdfsd', all_tokens_merge[10, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "res = generate([exp_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're studying self-attention heads in transformer large language model. Each head looks for some particular relationship between tokens in a short document. Look at the attention matrix for the part of the document and summarize in a single sentence what the head is looking for. Don't list examples of words.\n",
      "\n",
      "The attention matrix format is tokenA -> tokenB<tab>attention_score. Attention score values range from 0 to 100. A head finding what it's looking for is represented by a non-zero attention score. The higher the attention score, the stronger the match.\n",
      "\n",
      "Head 1\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "The ->  \t5\n",
      "\n",
      "other ->  \t5\n",
      "other -> The\t5\n",
      "\n",
      "men ->  \t0\n",
      "men -> The\t0\n",
      "men -> other\t0\n",
      "\n",
      "sh ->  \t0\n",
      "sh -> The\t0\n",
      "sh -> other\t0\n",
      "sh -> men\t99\n",
      "\n",
      "uff ->  \t0\n",
      "uff -> The\t0\n",
      "uff -> other\t0\n",
      "uff -> men\t92\n",
      "uff -> sh\t6\n",
      "\n",
      "led ->  \t0\n",
      "led -> The\t0\n",
      "led -> other\t0\n",
      "led -> men\t91\n",
      "led -> sh\t2\n",
      "led -> uff\t1\n",
      "\n",
      ". ->  \t0\n",
      ". -> The\t0\n",
      ". -> other\t0\n",
      ". -> men\t94\n",
      ". -> sh\t1\n",
      ". -> uff\t0\n",
      ". -> led\t2\n",
      "\n",
      "The ->  \t0\n",
      "The -> The\t0\n",
      "The -> other\t0\n",
      "The -> men\t67\n",
      "The -> sh\t0\n",
      "The -> uff\t2\n",
      "The -> led\t7\n",
      "The -> .\t7\n",
      "\n",
      "other ->  \t0\n",
      "other -> The\t0\n",
      "other -> other\t0\n",
      "other -> men\t85\n",
      "other -> sh\t0\n",
      "other -> uff\t0\n",
      "other -> led\t5\n",
      "other -> .\t3\n",
      "other -> The\t0\n",
      "\n",
      "men ->  \t0\n",
      "men -> The\t0\n",
      "men -> other\t0\n",
      "men -> men\t92\n",
      "men -> sh\t0\n",
      "men -> uff\t0\n",
      "men -> led\t2\n",
      "men -> .\t1\n",
      "men -> The\t0\n",
      "men -> other\t0\n",
      "\n",
      "were ->  \t0\n",
      "were -> The\t0\n",
      "were -> other\t0\n",
      "were -> men\t54\n",
      "were -> sh\t5\n",
      "were -> uff\t3\n",
      "were -> led\t3\n",
      "were -> .\t6\n",
      "were -> The\t1\n",
      "were -> other\t6\n",
      "were -> men\t16\n",
      "\n",
      "sh ->  \t0\n",
      "sh -> The\t0\n",
      "sh -> other\t0\n",
      "sh -> men\t72\n",
      "sh -> sh\t3\n",
      "sh -> uff\t2\n",
      "sh -> led\t5\n",
      "sh -> .\t2\n",
      "sh -> The\t1\n",
      "sh -> other\t2\n",
      "sh -> men\t4\n",
      "sh -> were\t2\n",
      "\n",
      "uff ->  \t0\n",
      "uff -> The\t0\n",
      "uff -> other\t0\n",
      "uff -> men\t70\n",
      "uff -> sh\t3\n",
      "uff -> uff\t1\n",
      "uff -> led\t3\n",
      "uff -> .\t3\n",
      "uff -> The\t0\n",
      "uff -> other\t4\n",
      "uff -> men\t5\n",
      "uff -> were\t2\n",
      "uff -> sh\t1\n",
      "\n",
      "led ->  \t0\n",
      "led -> The\t0\n",
      "led -> other\t0\n",
      "led -> men\t62\n",
      "led -> sh\t11\n",
      "led -> uff\t6\n",
      "led -> led\t2\n",
      "led -> .\t4\n",
      "led -> The\t0\n",
      "led -> other\t2\n",
      "led -> men\t2\n",
      "led -> were\t1\n",
      "led -> sh\t1\n",
      "led -> uff\t3\n",
      "\n",
      "around ->  \t0\n",
      "around -> The\t0\n",
      "around -> other\t0\n",
      "around -> men\t40\n",
      "around -> sh\t2\n",
      "around -> uff\t2\n",
      "around -> led\t2\n",
      "around -> .\t4\n",
      "around -> The\t3\n",
      "around -> other\t10\n",
      "around -> men\t23\n",
      "around -> were\t2\n",
      "around -> sh\t1\n",
      "around -> uff\t2\n",
      "around -> led\t2\n",
      "\n",
      ". ->  \t0\n",
      ". -> The\t0\n",
      ". -> other\t0\n",
      ". -> men\t35\n",
      ". -> sh\t1\n",
      ". -> uff\t1\n",
      ". -> led\t0\n",
      ". -> .\t0\n",
      ". -> The\t5\n",
      ". -> other\t3\n",
      ". -> men\t9\n",
      ". -> were\t3\n",
      ". -> sh\t5\n",
      ". -> uff\t9\n",
      ". -> led\t7\n",
      ". -> around\t10\n",
      "\n",
      "<end>\n",
      "\n",
      "Explanation of head 1 behavior: the main thing this head does is find relationship related to first subject.\n",
      "\n",
      "Head 2\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "And ->  \t0\n",
      "\n",
      "those ->  \t0\n",
      "those -> And\t98\n",
      "\n",
      "are ->  \t0\n",
      "are -> And\t93\n",
      "are -> those\t4\n",
      "\n",
      "just ->  \t0\n",
      "just -> And\t70\n",
      "just -> those\t3\n",
      "just -> are\t23\n",
      "\n",
      "the ->  \t0\n",
      "the -> And\t72\n",
      "the -> those\t2\n",
      "the -> are\t8\n",
      "the -> just\t2\n",
      "\n",
      "towns ->  \t0\n",
      "towns -> And\t56\n",
      "towns -> those\t1\n",
      "towns -> are\t31\n",
      "towns -> just\t6\n",
      "towns -> the\t2\n",
      "\n",
      "along ->  \t0\n",
      "along -> And\t76\n",
      "along -> those\t3\n",
      "along -> are\t7\n",
      "along -> just\t2\n",
      "along -> the\t3\n",
      "along -> towns\t4\n",
      "\n",
      "Lake ->  \t0\n",
      "Lake -> And\t64\n",
      "Lake -> those\t1\n",
      "Lake -> are\t20\n",
      "Lake -> just\t2\n",
      "Lake -> the\t1\n",
      "Lake -> towns\t1\n",
      "Lake -> along\t8\n",
      "\n",
      "Como ->  \t0\n",
      "Como -> And\t73\n",
      "Como -> those\t4\n",
      "Como -> are\t5\n",
      "Como -> just\t0\n",
      "Como -> the\t2\n",
      "Como -> towns\t3\n",
      "Como -> along\t4\n",
      "Como -> Lake\t3\n",
      "\n",
      ". ->  \t0\n",
      ". -> And\t51\n",
      ". -> those\t8\n",
      ". -> are\t24\n",
      ". -> just\t4\n",
      ". -> the\t2\n",
      ". -> towns\t0\n",
      ". -> along\t2\n",
      ". -> Lake\t0\n",
      ". -> Como\t1\n",
      "\n",
      "Those ->  \t0\n",
      "Those -> And\t60\n",
      "Those -> those\t3\n",
      "Those -> are\t5\n",
      "Those -> just\t2\n",
      "Those -> the\t1\n",
      "Those -> towns\t0\n",
      "Those -> along\t3\n",
      "Those -> Lake\t0\n",
      "Those -> Como\t5\n",
      "Those -> .\t8\n",
      "\n",
      "are ->  \t0\n",
      "are -> And\t43\n",
      "are -> those\t2\n",
      "are -> are\t7\n",
      "are -> just\t4\n",
      "are -> the\t1\n",
      "are -> towns\t0\n",
      "are -> along\t4\n",
      "are -> Lake\t0\n",
      "are -> Como\t6\n",
      "are -> .\t15\n",
      "are -> Those\t8\n",
      "\n",
      "the ->  \t0\n",
      "the -> And\t41\n",
      "the -> those\t4\n",
      "the -> are\t20\n",
      "the -> just\t3\n",
      "the -> the\t3\n",
      "the -> towns\t0\n",
      "the -> along\t5\n",
      "the -> Lake\t0\n",
      "the -> Como\t3\n",
      "the -> .\t6\n",
      "the -> Those\t4\n",
      "the -> are\t3\n",
      "\n",
      "towns ->  \t0\n",
      "towns -> And\t71\n",
      "towns -> those\t0\n",
      "towns -> are\t0\n",
      "towns -> just\t0\n",
      "towns -> the\t0\n",
      "towns -> towns\t0\n",
      "towns -> along\t0\n",
      "towns -> Lake\t0\n",
      "towns -> Como\t0\n",
      "towns -> .\t0\n",
      "towns -> Those\t1\n",
      "towns -> are\t0\n",
      "towns -> the\t0\n",
      "\n",
      "beside ->  \t0\n",
      "beside -> And\t63\n",
      "beside -> those\t1\n",
      "beside -> are\t0\n",
      "beside -> just\t0\n",
      "beside -> the\t0\n",
      "beside -> towns\t0\n",
      "beside -> along\t0\n",
      "beside -> Lake\t0\n",
      "beside -> Como\t1\n",
      "beside -> .\t5\n",
      "beside -> Those\t2\n",
      "beside -> are\t1\n",
      "beside -> the\t1\n",
      "beside -> towns\t14\n",
      "\n",
      "Lake ->  \t0\n",
      "Lake -> And\t47\n",
      "Lake -> those\t1\n",
      "Lake -> are\t0\n",
      "Lake -> just\t0\n",
      "Lake -> the\t0\n",
      "Lake -> towns\t0\n",
      "Lake -> along\t4\n",
      "Lake -> Lake\t0\n",
      "Lake -> Como\t5\n",
      "Lake -> .\t9\n",
      "Lake -> Those\t6\n",
      "Lake -> are\t0\n",
      "Lake -> the\t0\n",
      "Lake -> towns\t6\n",
      "Lake -> beside\t3\n",
      "\n",
      "Como ->  \t0\n",
      "Como -> And\t41\n",
      "Como -> those\t1\n",
      "Como -> are\t0\n",
      "Como -> just\t0\n",
      "Como -> the\t0\n",
      "Como -> towns\t0\n",
      "Como -> along\t2\n",
      "Como -> Lake\t0\n",
      "Como -> Como\t3\n",
      "Como -> .\t7\n",
      "Como -> Those\t8\n",
      "Como -> are\t2\n",
      "Como -> the\t0\n",
      "Como -> towns\t4\n",
      "Como -> beside\t4\n",
      "Como -> Lake\t6\n",
      "\n",
      ". ->  \t0\n",
      ". -> And\t56\n",
      ". -> those\t0\n",
      ". -> are\t0\n",
      ". -> just\t0\n",
      ". -> the\t0\n",
      ". -> towns\t0\n",
      ". -> along\t1\n",
      ". -> Lake\t0\n",
      ". -> Como\t1\n",
      ". -> .\t1\n",
      ". -> Those\t0\n",
      ". -> are\t2\n",
      ". -> the\t0\n",
      ". -> towns\t14\n",
      ". -> beside\t2\n",
      ". -> Lake\t1\n",
      ". -> Como\t8\n",
      "\n",
      " ->  \t0\n",
      " -> And\t55\n",
      " -> those\t0\n",
      " -> are\t0\n",
      " -> just\t0\n",
      " -> the\t0\n",
      " -> towns\t0\n",
      " -> along\t1\n",
      " -> Lake\t0\n",
      " -> Como\t2\n",
      " -> .\t0\n",
      " -> Those\t0\n",
      " -> are\t1\n",
      " -> the\t0\n",
      " -> towns\t19\n",
      " -> beside\t1\n",
      " -> Lake\t1\n",
      " -> Como\t3\n",
      " -> .\t1\n",
      "\n",
      "<end>\n",
      "\n",
      "Explanation of head 2 behavior: the main thing this head does is find relationship related to capital article.\n",
      "\n",
      "Head 3\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "' ->  \t5\n",
      "\n",
      "Att ->  \t5\n",
      "Att -> '\t5\n",
      "\n",
      "ention ->  \t5\n",
      "ention -> '\t5\n",
      "ention -> Att\t5\n",
      "\n",
      ", ->  \t5\n",
      ", -> '\t5\n",
      ", -> Att\t5\n",
      ", -> ention\t5\n",
      "\n",
      "ladies ->  \t0\n",
      "ladies -> '\t0\n",
      "ladies -> Att\t0\n",
      "ladies -> ention\t0\n",
      "ladies -> ,\t0\n",
      "\n",
      "and ->  \t0\n",
      "and -> '\t0\n",
      "and -> Att\t0\n",
      "and -> ention\t0\n",
      "and -> ,\t0\n",
      "and -> ladies\t52\n",
      "\n",
      "gentlemen ->  \t0\n",
      "gentlemen -> '\t0\n",
      "gentlemen -> Att\t0\n",
      "gentlemen -> ention\t0\n",
      "gentlemen -> ,\t0\n",
      "gentlemen -> ladies\t61\n",
      "gentlemen -> and\t6\n",
      "\n",
      ". ->  \t0\n",
      ". -> '\t0\n",
      ". -> Att\t0\n",
      ". -> ention\t0\n",
      ". -> ,\t0\n",
      ". -> ladies\t43\n",
      ". -> and\t29\n",
      ". -> gentlemen\t27\n",
      "\n",
      "The ->  \t0\n",
      "The -> '\t0\n",
      "The -> Att\t0\n",
      "The -> ention\t0\n",
      "The -> ,\t0\n",
      "The -> ladies\t2\n",
      "The -> and\t7\n",
      "The -> gentlemen\t10\n",
      "The -> .\t31\n",
      "\n",
      "announ ->  \t0\n",
      "announ -> '\t0\n",
      "announ -> Att\t0\n",
      "announ -> ention\t0\n",
      "announ -> ,\t0\n",
      "announ -> ladies\t74\n",
      "announ -> and\t0\n",
      "announ -> gentlemen\t0\n",
      "announ -> .\t1\n",
      "announ -> The\t23\n",
      "\n",
      "cement ->  \t0\n",
      "cement -> '\t0\n",
      "cement -> Att\t0\n",
      "cement -> ention\t0\n",
      "cement -> ,\t0\n",
      "cement -> ladies\t1\n",
      "cement -> and\t0\n",
      "cement -> gentlemen\t0\n",
      "cement -> .\t2\n",
      "cement -> The\t19\n",
      "cement -> announ\t9\n",
      "\n",
      "was ->  \t0\n",
      "was -> '\t0\n",
      "was -> Att\t0\n",
      "was -> ention\t0\n",
      "was -> ,\t0\n",
      "was -> ladies\t76\n",
      "was -> and\t0\n",
      "was -> gentlemen\t0\n",
      "was -> .\t0\n",
      "was -> The\t5\n",
      "was -> announ\t0\n",
      "was -> cement\t18\n",
      "\n",
      "important ->  \t0\n",
      "important -> '\t0\n",
      "important -> Att\t0\n",
      "important -> ention\t0\n",
      "important -> ,\t0\n",
      "important -> ladies\t0\n",
      "important -> and\t0\n",
      "important -> gentlemen\t0\n",
      "important -> .\t10\n",
      "important -> The\t26\n",
      "important -> announ\t34\n",
      "important -> cement\t6\n",
      "important -> was\t21\n",
      "\n",
      ". ->  \t0\n",
      ". -> '\t0\n",
      ". -> Att\t0\n",
      ". -> ention\t0\n",
      ". -> ,\t0\n",
      ". -> ladies\t59\n",
      ". -> and\t10\n",
      ". -> gentlemen\t15\n",
      ". -> .\t3\n",
      ". -> The\t4\n",
      ". -> announ\t1\n",
      ". -> cement\t2\n",
      ". -> was\t1\n",
      ". -> important\t0\n",
      "\n",
      "<end>\n",
      "\n",
      "Explanation of head 3 behavior: the main thing this head does is find relationship related to the main characters.\n",
      "\n",
      "Head 4\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "You ->  \t5\n",
      "\n",
      "have ->  \t5\n",
      "have -> You\t5\n",
      "\n",
      "access ->  \t5\n",
      "access -> You\t5\n",
      "access -> have\t5\n",
      "\n",
      "to ->  \t0\n",
      "to -> You\t0\n",
      "to -> have\t0\n",
      "to -> access\t0\n",
      "\n",
      "the ->  \t0\n",
      "the -> You\t0\n",
      "the -> have\t0\n",
      "the -> access\t0\n",
      "the -> to\t99\n",
      "\n",
      "facts ->  \t0\n",
      "facts -> You\t0\n",
      "facts -> have\t0\n",
      "facts -> access\t0\n",
      "facts -> to\t91\n",
      "facts -> the\t1\n",
      "\n",
      ". ->  \t0\n",
      ". -> You\t0\n",
      ". -> have\t0\n",
      ". -> access\t0\n",
      ". -> to\t92\n",
      ". -> the\t0\n",
      ". -> facts\t0\n",
      "\n",
      " ->  \t0\n",
      " -> You\t0\n",
      " -> have\t0\n",
      " -> access\t0\n",
      " -> to\t93\n",
      " -> the\t0\n",
      " -> facts\t0\n",
      " -> .\t1\n",
      "\n",
      "The ->  \t0\n",
      "The -> You\t0\n",
      "The -> have\t0\n",
      "The -> access\t0\n",
      "The -> to\t96\n",
      "The -> the\t0\n",
      "The -> facts\t0\n",
      "The -> .\t0\n",
      "The -> \t1\n",
      "\n",
      "facts ->  \t0\n",
      "facts -> You\t0\n",
      "facts -> have\t0\n",
      "facts -> access\t0\n",
      "facts -> to\t96\n",
      "facts -> the\t0\n",
      "facts -> facts\t0\n",
      "facts -> .\t0\n",
      "facts -> \t0\n",
      "facts -> The\t0\n",
      "\n",
      "are ->  \t0\n",
      "are -> You\t0\n",
      "are -> have\t0\n",
      "are -> access\t0\n",
      "are -> to\t98\n",
      "are -> the\t0\n",
      "are -> facts\t0\n",
      "are -> .\t0\n",
      "are -> \t0\n",
      "are -> The\t0\n",
      "are -> facts\t0\n",
      "\n",
      "accessible ->  \t0\n",
      "accessible -> You\t0\n",
      "accessible -> have\t0\n",
      "accessible -> access\t0\n",
      "accessible -> to\t83\n",
      "accessible -> the\t0\n",
      "accessible -> facts\t0\n",
      "accessible -> .\t0\n",
      "accessible -> \t0\n",
      "accessible -> The\t0\n",
      "accessible -> facts\t1\n",
      "accessible -> are\t1\n",
      "\n",
      "to ->  \t0\n",
      "to -> You\t0\n",
      "to -> have\t0\n",
      "to -> access\t0\n",
      "to -> to\t97\n",
      "to -> the\t0\n",
      "to -> facts\t0\n",
      "to -> .\t0\n",
      "to -> \t0\n",
      "to -> The\t0\n",
      "to -> facts\t0\n",
      "to -> are\t0\n",
      "to -> accessible\t0\n",
      "\n",
      "you ->  \t0\n",
      "you -> You\t0\n",
      "you -> have\t0\n",
      "you -> access\t0\n",
      "you -> to\t99\n",
      "you -> the\t0\n",
      "you -> facts\t0\n",
      "you -> .\t0\n",
      "you -> \t0\n",
      "you -> The\t0\n",
      "you -> facts\t0\n",
      "you -> are\t0\n",
      "you -> accessible\t0\n",
      "you -> to\t0\n",
      "\n",
      ". ->  \t0\n",
      ". -> You\t0\n",
      ". -> have\t0\n",
      ". -> access\t0\n",
      ". -> to\t97\n",
      ". -> the\t0\n",
      ". -> facts\t0\n",
      ". -> .\t0\n",
      ". -> \t0\n",
      ". -> The\t0\n",
      ". -> facts\t0\n",
      ". -> are\t0\n",
      ". -> accessible\t0\n",
      ". -> to\t0\n",
      ". -> you\t0\n",
      "\n",
      "<end>\n",
      "\n",
      "Explanation of head 4 behavior: the main thing this head does is find relationship  to the object of interest\n",
      "Head 5\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "have -> 0 \t8\n",
      "access -> 0\t5\n",
      "access -> have\t2\n",
      "access -> access\t0\n",
      "\n",
      "to -> 0\t2\n",
      "to -> access\t81\n",
      "to -> have\t0\n",
      "to -> to\t5\n",
      "to -> access\t1\n",
      "to -> have\t0\n",
      "to -> access\t1\n",
      "\n",
      "have -> 0\t0\n",
      "have -> have\t4\n",
      "have -> access\t0\n",
      "have -> to\t96\n",
      "have -> have\t6\n",
      "have -> to\t0\n",
      "have -> access\t0\n",
      "have -> have\t1\n",
      "have -> access\t0\n",
      "\n",
      "accessible -> 0\t0\n",
      "accessible -> access\t0\n",
      "accessible -> have\t23\n",
      "accessible -> to\t99\n",
      "accessible -> have\t25\n",
      "accessible -> access\t12\n",
      "accessible ->\n"
     ]
    }
   ],
   "source": [
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "a = generate([sim_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're studying self-attention heads in transformer large language model. Each head looks for some particular relationship between tokens in a short document. Look at an explanation of what the head does, and try to predict how it will split attention between tokens.\n",
      "\n",
      "The attention matrix format is tokenA -> tokenB<tab>attention_score. Attention score values range from 0 to 100, \"unknown\" indicates an unknown attention score. A head finding what it's looking for is represented by a non-zero attention score. The higher the attention score, the stronger the match.\n",
      "\n",
      "Head 1\n",
      "Explanation of head 1 behavior: the main thing this head does is find relationship related to first subject.\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "The ->  \t5\n",
      "\n",
      "other ->  \t5\n",
      "other -> The\t5\n",
      "\n",
      "men ->  \t0\n",
      "men -> The\t0\n",
      "men -> other\t0\n",
      "\n",
      "sh ->  \t0\n",
      "sh -> The\t0\n",
      "sh -> other\t0\n",
      "sh -> men\t99\n",
      "\n",
      "uff ->  \t0\n",
      "uff -> The\t0\n",
      "uff -> other\t0\n",
      "uff -> men\t92\n",
      "uff -> sh\t6\n",
      "\n",
      "led ->  \t0\n",
      "led -> The\t0\n",
      "led -> other\t0\n",
      "led -> men\t91\n",
      "led -> sh\t2\n",
      "led -> uff\t1\n",
      "\n",
      ". ->  \t0\n",
      ". -> The\t0\n",
      ". -> other\t0\n",
      ". -> men\t94\n",
      ". -> sh\t1\n",
      ". -> uff\t0\n",
      ". -> led\t2\n",
      "\n",
      "The ->  \t0\n",
      "The -> The\t0\n",
      "The -> other\t0\n",
      "The -> men\t67\n",
      "The -> sh\t0\n",
      "The -> uff\t2\n",
      "The -> led\t7\n",
      "The -> .\t7\n",
      "\n",
      "other ->  \t0\n",
      "other -> The\t0\n",
      "other -> other\t0\n",
      "other -> men\t85\n",
      "other -> sh\t0\n",
      "other -> uff\t0\n",
      "other -> led\t5\n",
      "other -> .\t3\n",
      "other -> The\t0\n",
      "\n",
      "men ->  \t0\n",
      "men -> The\t0\n",
      "men -> other\t0\n",
      "men -> men\t92\n",
      "men -> sh\t0\n",
      "men -> uff\t0\n",
      "men -> led\t2\n",
      "men -> .\t1\n",
      "men -> The\t0\n",
      "men -> other\t0\n",
      "\n",
      "were ->  \t0\n",
      "were -> The\t0\n",
      "were -> other\t0\n",
      "were -> men\t54\n",
      "were -> sh\t5\n",
      "were -> uff\t3\n",
      "were -> led\t3\n",
      "were -> .\t6\n",
      "were -> The\t1\n",
      "were -> other\t6\n",
      "were -> men\t16\n",
      "\n",
      "sh ->  \t0\n",
      "sh -> The\t0\n",
      "sh -> other\t0\n",
      "sh -> men\t72\n",
      "sh -> sh\t3\n",
      "sh -> uff\t2\n",
      "sh -> led\t5\n",
      "sh -> .\t2\n",
      "sh -> The\t1\n",
      "sh -> other\t2\n",
      "sh -> men\t4\n",
      "sh -> were\t2\n",
      "\n",
      "uff ->  \t0\n",
      "uff -> The\t0\n",
      "uff -> other\t0\n",
      "uff -> men\t70\n",
      "uff -> sh\t3\n",
      "uff -> uff\t1\n",
      "uff -> led\t3\n",
      "uff -> .\t3\n",
      "uff -> The\t0\n",
      "uff -> other\t4\n",
      "uff -> men\t5\n",
      "uff -> were\t2\n",
      "uff -> sh\t1\n",
      "\n",
      "led ->  \t0\n",
      "led -> The\t0\n",
      "led -> other\t0\n",
      "led -> men\t62\n",
      "led -> sh\t11\n",
      "led -> uff\t6\n",
      "led -> led\t2\n",
      "led -> .\t4\n",
      "led -> The\t0\n",
      "led -> other\t2\n",
      "led -> men\t2\n",
      "led -> were\t1\n",
      "led -> sh\t1\n",
      "led -> uff\t3\n",
      "\n",
      "around ->  \t0\n",
      "around -> The\t0\n",
      "around -> other\t0\n",
      "around -> men\t40\n",
      "around -> sh\t2\n",
      "around -> uff\t2\n",
      "around -> led\t2\n",
      "around -> .\t4\n",
      "around -> The\t3\n",
      "around -> other\t10\n",
      "around -> men\t23\n",
      "around -> were\t2\n",
      "around -> sh\t1\n",
      "around -> uff\t2\n",
      "around -> led\t2\n",
      "\n",
      ". ->  \t0\n",
      ". -> The\t0\n",
      ". -> other\t0\n",
      ". -> men\t35\n",
      ". -> sh\t1\n",
      ". -> uff\t1\n",
      ". -> led\t0\n",
      ". -> .\t0\n",
      ". -> The\t5\n",
      ". -> other\t3\n",
      ". -> men\t9\n",
      ". -> were\t3\n",
      ". -> sh\t5\n",
      ". -> uff\t9\n",
      ". -> led\t7\n",
      ". -> around\t10\n",
      "\n",
      "<end>\n",
      "\n",
      "Head 2\n",
      "Explanation of head 2 behavior: the main thing this head does is find relationship related to capital article.\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "And ->  \t0\n",
      "\n",
      "those ->  \t0\n",
      "those -> And\t98\n",
      "\n",
      "are ->  \t0\n",
      "are -> And\t93\n",
      "are -> those\t4\n",
      "\n",
      "just ->  \t0\n",
      "just -> And\t70\n",
      "just -> those\t3\n",
      "just -> are\t23\n",
      "\n",
      "the ->  \t0\n",
      "the -> And\t72\n",
      "the -> those\t2\n",
      "the -> are\t8\n",
      "the -> just\t2\n",
      "\n",
      "towns ->  \t0\n",
      "towns -> And\t56\n",
      "towns -> those\t1\n",
      "towns -> are\t31\n",
      "towns -> just\t6\n",
      "towns -> the\t2\n",
      "\n",
      "along ->  \t0\n",
      "along -> And\t76\n",
      "along -> those\t3\n",
      "along -> are\t7\n",
      "along -> just\t2\n",
      "along -> the\t3\n",
      "along -> towns\t4\n",
      "\n",
      "Lake ->  \t0\n",
      "Lake -> And\t64\n",
      "Lake -> those\t1\n",
      "Lake -> are\t20\n",
      "Lake -> just\t2\n",
      "Lake -> the\t1\n",
      "Lake -> towns\t1\n",
      "Lake -> along\t8\n",
      "\n",
      "Como ->  \t0\n",
      "Como -> And\t73\n",
      "Como -> those\t4\n",
      "Como -> are\t5\n",
      "Como -> just\t0\n",
      "Como -> the\t2\n",
      "Como -> towns\t3\n",
      "Como -> along\t4\n",
      "Como -> Lake\t3\n",
      "\n",
      ". ->  \t0\n",
      ". -> And\t51\n",
      ". -> those\t8\n",
      ". -> are\t24\n",
      ". -> just\t4\n",
      ". -> the\t2\n",
      ". -> towns\t0\n",
      ". -> along\t2\n",
      ". -> Lake\t0\n",
      ". -> Como\t1\n",
      "\n",
      "Those ->  \t0\n",
      "Those -> And\t60\n",
      "Those -> those\t3\n",
      "Those -> are\t5\n",
      "Those -> just\t2\n",
      "Those -> the\t1\n",
      "Those -> towns\t0\n",
      "Those -> along\t3\n",
      "Those -> Lake\t0\n",
      "Those -> Como\t5\n",
      "Those -> .\t8\n",
      "\n",
      "are ->  \t0\n",
      "are -> And\t43\n",
      "are -> those\t2\n",
      "are -> are\t7\n",
      "are -> just\t4\n",
      "are -> the\t1\n",
      "are -> towns\t0\n",
      "are -> along\t4\n",
      "are -> Lake\t0\n",
      "are -> Como\t6\n",
      "are -> .\t15\n",
      "are -> Those\t8\n",
      "\n",
      "the ->  \t0\n",
      "the -> And\t41\n",
      "the -> those\t4\n",
      "the -> are\t20\n",
      "the -> just\t3\n",
      "the -> the\t3\n",
      "the -> towns\t0\n",
      "the -> along\t5\n",
      "the -> Lake\t0\n",
      "the -> Como\t3\n",
      "the -> .\t6\n",
      "the -> Those\t4\n",
      "the -> are\t3\n",
      "\n",
      "towns ->  \t0\n",
      "towns -> And\t71\n",
      "towns -> those\t0\n",
      "towns -> are\t0\n",
      "towns -> just\t0\n",
      "towns -> the\t0\n",
      "towns -> towns\t0\n",
      "towns -> along\t0\n",
      "towns -> Lake\t0\n",
      "towns -> Como\t0\n",
      "towns -> .\t0\n",
      "towns -> Those\t1\n",
      "towns -> are\t0\n",
      "towns -> the\t0\n",
      "\n",
      "beside ->  \t0\n",
      "beside -> And\t63\n",
      "beside -> those\t1\n",
      "beside -> are\t0\n",
      "beside -> just\t0\n",
      "beside -> the\t0\n",
      "beside -> towns\t0\n",
      "beside -> along\t0\n",
      "beside -> Lake\t0\n",
      "beside -> Como\t1\n",
      "beside -> .\t5\n",
      "beside -> Those\t2\n",
      "beside -> are\t1\n",
      "beside -> the\t1\n",
      "beside -> towns\t14\n",
      "\n",
      "Lake ->  \t0\n",
      "Lake -> And\t47\n",
      "Lake -> those\t1\n",
      "Lake -> are\t0\n",
      "Lake -> just\t0\n",
      "Lake -> the\t0\n",
      "Lake -> towns\t0\n",
      "Lake -> along\t4\n",
      "Lake -> Lake\t0\n",
      "Lake -> Como\t5\n",
      "Lake -> .\t9\n",
      "Lake -> Those\t6\n",
      "Lake -> are\t0\n",
      "Lake -> the\t0\n",
      "Lake -> towns\t6\n",
      "Lake -> beside\t3\n",
      "\n",
      "Como ->  \t0\n",
      "Como -> And\t41\n",
      "Como -> those\t1\n",
      "Como -> are\t0\n",
      "Como -> just\t0\n",
      "Como -> the\t0\n",
      "Como -> towns\t0\n",
      "Como -> along\t2\n",
      "Como -> Lake\t0\n",
      "Como -> Como\t3\n",
      "Como -> .\t7\n",
      "Como -> Those\t8\n",
      "Como -> are\t2\n",
      "Como -> the\t0\n",
      "Como -> towns\t4\n",
      "Como -> beside\t4\n",
      "Como -> Lake\t6\n",
      "\n",
      ". ->  \t0\n",
      ". -> And\t56\n",
      ". -> those\t0\n",
      ". -> are\t0\n",
      ". -> just\t0\n",
      ". -> the\t0\n",
      ". -> towns\t0\n",
      ". -> along\t1\n",
      ". -> Lake\t0\n",
      ". -> Como\t1\n",
      ". -> .\t1\n",
      ". -> Those\t0\n",
      ". -> are\t2\n",
      ". -> the\t0\n",
      ". -> towns\t14\n",
      ". -> beside\t2\n",
      ". -> Lake\t1\n",
      ". -> Como\t8\n",
      "\n",
      " ->  \t0\n",
      " -> And\t55\n",
      " -> those\t0\n",
      " -> are\t0\n",
      " -> just\t0\n",
      " -> the\t0\n",
      " -> towns\t0\n",
      " -> along\t1\n",
      " -> Lake\t0\n",
      " -> Como\t2\n",
      " -> .\t0\n",
      " -> Those\t0\n",
      " -> are\t1\n",
      " -> the\t0\n",
      " -> towns\t19\n",
      " -> beside\t1\n",
      " -> Lake\t1\n",
      " -> Como\t3\n",
      " -> .\t1\n",
      "\n",
      "<end>\n",
      "\n",
      "Head 3\n",
      "Explanation of head 3 behavior: the main thing this head does is find relationship related to the main characters.\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "' ->  \t5\n",
      "\n",
      "Att ->  \t5\n",
      "Att -> '\t5\n",
      "\n",
      "ention ->  \t5\n",
      "ention -> '\t5\n",
      "ention -> Att\t5\n",
      "\n",
      ", ->  \t5\n",
      ", -> '\t5\n",
      ", -> Att\t5\n",
      ", -> ention\t5\n",
      "\n",
      "ladies ->  \t0\n",
      "ladies -> '\t0\n",
      "ladies -> Att\t0\n",
      "ladies -> ention\t0\n",
      "ladies -> ,\t0\n",
      "\n",
      "and ->  \t0\n",
      "and -> '\t0\n",
      "and -> Att\t0\n",
      "and -> ention\t0\n",
      "and -> ,\t0\n",
      "and -> ladies\t52\n",
      "\n",
      "gentlemen ->  \t0\n",
      "gentlemen -> '\t0\n",
      "gentlemen -> Att\t0\n",
      "gentlemen -> ention\t0\n",
      "gentlemen -> ,\t0\n",
      "gentlemen -> ladies\t61\n",
      "gentlemen -> and\t6\n",
      "\n",
      ". ->  \t0\n",
      ". -> '\t0\n",
      ". -> Att\t0\n",
      ". -> ention\t0\n",
      ". -> ,\t0\n",
      ". -> ladies\t43\n",
      ". -> and\t29\n",
      ". -> gentlemen\t27\n",
      "\n",
      "The ->  \t0\n",
      "The -> '\t0\n",
      "The -> Att\t0\n",
      "The -> ention\t0\n",
      "The -> ,\t0\n",
      "The -> ladies\t2\n",
      "The -> and\t7\n",
      "The -> gentlemen\t10\n",
      "The -> .\t31\n",
      "\n",
      "announ ->  \t0\n",
      "announ -> '\t0\n",
      "announ -> Att\t0\n",
      "announ -> ention\t0\n",
      "announ -> ,\t0\n",
      "announ -> ladies\t74\n",
      "announ -> and\t0\n",
      "announ -> gentlemen\t0\n",
      "announ -> .\t1\n",
      "announ -> The\t23\n",
      "\n",
      "cement ->  \t0\n",
      "cement -> '\t0\n",
      "cement -> Att\t0\n",
      "cement -> ention\t0\n",
      "cement -> ,\t0\n",
      "cement -> ladies\t1\n",
      "cement -> and\t0\n",
      "cement -> gentlemen\t0\n",
      "cement -> .\t2\n",
      "cement -> The\t19\n",
      "cement -> announ\t9\n",
      "\n",
      "was ->  \t0\n",
      "was -> '\t0\n",
      "was -> Att\t0\n",
      "was -> ention\t0\n",
      "was -> ,\t0\n",
      "was -> ladies\t76\n",
      "was -> and\t0\n",
      "was -> gentlemen\t0\n",
      "was -> .\t0\n",
      "was -> The\t5\n",
      "was -> announ\t0\n",
      "was -> cement\t18\n",
      "\n",
      "important ->  \t0\n",
      "important -> '\t0\n",
      "important -> Att\t0\n",
      "important -> ention\t0\n",
      "important -> ,\t0\n",
      "important -> ladies\t0\n",
      "important -> and\t0\n",
      "important -> gentlemen\t0\n",
      "important -> .\t10\n",
      "important -> The\t26\n",
      "important -> announ\t34\n",
      "important -> cement\t6\n",
      "important -> was\t21\n",
      "\n",
      ". ->  \t0\n",
      ". -> '\t0\n",
      ". -> Att\t0\n",
      ". -> ention\t0\n",
      ". -> ,\t0\n",
      ". -> ladies\t59\n",
      ". -> and\t10\n",
      ". -> gentlemen\t15\n",
      ". -> .\t3\n",
      ". -> The\t4\n",
      ". -> announ\t1\n",
      ". -> cement\t2\n",
      ". -> was\t1\n",
      ". -> important\t0\n",
      "\n",
      "<end>\n",
      "\n",
      "Head 4\n",
      "Explanation of head 4 behavior: the main thing this head does is find relationship sdfssd sdfsd.\n",
      "Attention scores:\n",
      "<start>\n",
      "\n",
      "and ->  \tunknown\n",
      "\n",
      "it ->  \tunknown\n",
      "it -> and\tunknown\n",
      "\n",
      "is ->  \tunknown\n",
      "is -> and\tunknown\n",
      "is -> it\tunknown\n",
      "\n",
      "nice ->  \tunknown\n",
      "nice -> and\tunknown\n",
      "nice -> it\tunknown\n",
      "nice -> is\tunknown\n",
      "\n",
      "talking ->  \tunknown\n",
      "talking -> and\tunknown\n",
      "talking -> it\tunknown\n",
      "talking -> is\tunknown\n",
      "talking -> nice\tunknown\n",
      "\n",
      "to ->  \tunknown\n",
      "to -> and\tunknown\n",
      "to -> it\tunknown\n",
      "to -> is\tunknown\n",
      "to -> nice\tunknown\n",
      "to -> talking\tunknown\n",
      "\n",
      "you ->  \tunknown\n",
      "you -> and\tunknown\n",
      "you -> it\tunknown\n",
      "you -> is\tunknown\n",
      "you -> nice\tunknown\n",
      "you -> talking\tunknown\n",
      "you -> to\tunknown\n",
      "\n",
      "all ->  \tunknown\n",
      "all -> and\tunknown\n",
      "all -> it\tunknown\n",
      "all -> is\tunknown\n",
      "all -> nice\tunknown\n",
      "all -> talking\tunknown\n",
      "all -> to\tunknown\n",
      "all -> you\tunknown\n",
      "\n",
      "right ->  \tunknown\n",
      "right -> and\tunknown\n",
      "right -> it\tunknown\n",
      "right -> is\tunknown\n",
      "right -> nice\tunknown\n",
      "right -> talking\tunknown\n",
      "right -> to\tunknown\n",
      "right -> you\tunknown\n",
      "right -> all\tunknown\n",
      "\n",
      "y ->  \tunknown\n",
      "y -> and\tunknown\n",
      "y -> it\tunknown\n",
      "y -> is\tunknown\n",
      "y -> nice\tunknown\n",
      "y -> talking\tunknown\n",
      "y -> to\tunknown\n",
      "y -> you\tunknown\n",
      "y -> all\tunknown\n",
      "y -> right\tunknown\n",
      "\n",
      "I ->  \tunknown\n",
      "I -> and\tunknown\n",
      "I -> it\tunknown\n",
      "I -> is\tunknown\n",
      "I -> nice\tunknown\n",
      "I -> talking\tunknown\n",
      "I -> to\tunknown\n",
      "I -> you\tunknown\n",
      "I -> all\tunknown\n",
      "I -> right\tunknown\n",
      "I -> y\tunknown\n",
      "\n",
      "talk ->  \tunknown\n",
      "talk -> and\tunknown\n",
      "talk -> it\tunknown\n",
      "talk -> is\tunknown\n",
      "talk -> nice\tunknown\n",
      "talk -> talking\tunknown\n",
      "talk -> to\tunknown\n",
      "talk -> you\tunknown\n",
      "talk -> all\tunknown\n",
      "talk -> right\tunknown\n",
      "talk -> y\tunknown\n",
      "talk -> I\tunknown\n",
      "\n",
      "to ->  \tunknown\n",
      "to -> and\tunknown\n",
      "to -> it\tunknown\n",
      "to -> is\tunknown\n",
      "to -> nice\tunknown\n",
      "to -> talking\tunknown\n",
      "to -> to\tunknown\n",
      "to -> you\tunknown\n",
      "to -> all\tunknown\n",
      "to -> right\tunknown\n",
      "to -> y\tunknown\n",
      "to -> I\tunknown\n",
      "to -> talk\tunknown\n",
      "\n",
      "you ->  \tunknown\n",
      "you -> and\tunknown\n",
      "you -> it\tunknown\n",
      "you -> is\tunknown\n",
      "you -> nice\tunknown\n",
      "you -> talking\tunknown\n",
      "you -> to\tunknown\n",
      "you -> you\tunknown\n",
      "you -> all\tunknown\n",
      "you -> right\tunknown\n",
      "you -> y\tunknown\n",
      "you -> I\tunknown\n",
      "you -> talk\tunknown\n",
      "you -> to\tunknown\n",
      "\n",
      "every ->  \tunknown\n",
      "every -> and\tunknown\n",
      "every -> it\tunknown\n",
      "every -> is\tunknown\n",
      "every -> nice\tunknown\n",
      "every -> talking\tunknown\n",
      "every -> to\tunknown\n",
      "every -> you\tunknown\n",
      "every -> all\tunknown\n",
      "every -> right\tunknown\n",
      "every -> y\tunknown\n",
      "every -> I\tunknown\n",
      "every -> talk\tunknown\n",
      "every -> to\tunknown\n",
      "every -> you\tunknown\n",
      "\n",
      "day ->  \tunknown\n",
      "day -> and\tunknown\n",
      "day -> it\tunknown\n",
      "day -> is\tunknown\n",
      "day -> nice\tunknown\n",
      "day -> talking\tunknown\n",
      "day -> to\tunknown\n",
      "day -> you\tunknown\n",
      "day -> all\tunknown\n",
      "day -> right\tunknown\n",
      "day -> y\tunknown\n",
      "day -> I\tunknown\n",
      "day -> talk\tunknown\n",
      "day -> to\tunknown\n",
      "day -> you\tunknown\n",
      "day -> every\tunknown\n",
      "\n",
      ". ->  \tunknown\n",
      ". -> and\tunknown\n",
      ". -> it\tunknown\n",
      ". -> is\tunknown\n",
      ". -> nice\tunknown\n",
      ". -> talking\tunknown\n",
      ". -> to\tunknown\n",
      ". -> you\tunknown\n",
      ". -> all\tunknown\n",
      ". -> right\tunknown\n",
      ". -> y\tunknown\n",
      ". -> I\tunknown\n",
      ". -> talk\tunknown\n",
      ". -> to\tunknown\n",
      ". -> you\tunknown\n",
      ". -> every\tunknown\n",
      ". -> day\tunknown\n",
      "\n",
      "<end>\n",
      "Head 5\n",
      "Explanation of head 5 behavior: the main thing this head does is find relationship related to time.\n",
      "Attention scores:\n",
      "<start>\n",
      "and -> \t0\n",
      "and -> '\t9\n",
      "and -> Att\t0\n",
      "and -> ention\t0\n",
      "and -> ,\t0\n",
      "and -> ladies\t0\n",
      "and -> gentlemen\t0\n",
      "and -> .\t3\n",
      "and -> The\t0\n",
      ". -> \t0\n",
      ". -> '\t3\n",
      ". -> Att\t0\n",
      ". -> ention\t0\n",
      ". -> ,\t0\n",
      ". -> ladies\t0\n",
      ". -> gentlemen\t0\n",
      ". -> The\t0\n",
      "\n",
      "it -> \t1\n",
      "it -> '\t42\n",
      "it -> Att\t0\n",
      "it -> ention\t0\n",
      "it -> ,\t0\n",
      "it -> ladies\t105\n",
      "it -> and\t0\n",
      "it -> gentlemen\t88\n",
      "it ->\n"
     ]
    }
   ],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesya_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
